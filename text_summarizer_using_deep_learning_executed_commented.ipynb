{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9__vAdMylcN"
   },
   "source": [
    "In this Notebook we'll build an Text Summarizer using Deep Learning from Scratch using Python, This Summarizer uses Abstractive text summarization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5dSoP8lGMZi"
   },
   "source": [
    "#Understanding the Problem Statement\n",
    "\n",
    "Text related to Technology and can often be long and descriptive. Analyzing these manually, as you can imagine, is a tideous task. This is where we will use Natural Language Processing to make our task of understanding the long text more comfortably, here we will apply NLP to generate a summary for long Texts.\n",
    "\n",
    "We will be working on a really cool dataset. Our objective here is to generate a summary for the  text and more specifically Tech related using the abstractive Text Sumarization-based approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcPZzSnX0jmW"
   },
   "source": [
    "\n",
    "#Custom Attention Layer\n",
    "\n",
    "Keras does not officially support attention layer. So we are left with two choices we can either implement our own attention layer or use a third-party implementation. Since there will many compatibility issues when used thrid party attention layers we will implement our own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr26UtEibrmP",
    "outputId": "f55932c9-2cd5-4bb8-cc55-a8e29b06de03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keras-attention\n",
      "  Downloading keras_attention-1.0.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-attention) (2.9.0)\n",
      "Installing collected packages: keras-attention\n",
      "Successfully installed keras-attention-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IwKI-M8Ig4V"
   },
   "source": [
    "#Attention Layer\n",
    "\n",
    "Attention Mechanism is used when we want to give importance to certain words in the text than others that is we can add weights to some words in the text, so instead of looking at all the words in the input sequence our model can specifically look for such important words which can helkp us provide our output sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZRrvUaA1cXj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #importing tensorflow and keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "logger = tf.get_logger()\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):  #Creating attention layer class\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs): #constructor for AttentionLayer class\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # we will call this function  at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "\n",
    "        logger.debug(f\"encoder_out_seq.shape = {encoder_out_seq.shape}\")\n",
    "        logger.debug(f\"decoder_out_seq.shape = {decoder_out_seq.shape}\")\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            logger.debug(\"Running energy computation step\")\n",
    "\n",
    "            if not isinstance(states, (list, tuple)):\n",
    "                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n",
    "\n",
    "            encoder_full_seq = states[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_full_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "\n",
    "            logger.debug(f\"U_a_dot_h.shape = {U_a_dot_h.shape}\")\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "\n",
    "            logger.debug(f\"Ws_plus_Uh.shape = {Ws_plus_Uh.shape}\")\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            logger.debug(f\"ei.shape = {e_i.shape}\")\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            logger.debug(\"Running attention vector computation step\")\n",
    "\n",
    "            if not isinstance(states, (list, tuple)):\n",
    "                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n",
    "\n",
    "            encoder_full_seq = states[-1]\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_full_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "\n",
    "            logger.debug(f\"ci.shape = {c_i.shape}\")\n",
    "\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        # we don't maintain states between steps when computing attention\n",
    "        # attention is stateless, so we're passing a fake state for RNN step function\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e], constants=[encoder_out_seq]\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c], constants=[encoder_out_seq]\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUValOzcHtEK"
   },
   "source": [
    "#Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "_Jpu8qLEFxcY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVakjZ3oICgx"
   },
   "source": [
    "#Read the dataset\n",
    "\n",
    "This dataset contains text of more than 500,000\n",
    "We’ll take a sample of 200,000 reviews to reduce the training time of our model since our machine does not have that kind of computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhmArN8KelLF",
    "outputId": "ff27efee-f76a-478d-da62-af870513ae82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnK5o4Z1Fxcj"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/content/drive/MyDrive/NLP/School_text.csv\",nrows=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGNQKvCaISIn"
   },
   "source": [
    "# Drop Duplicates and NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cjul88oOFxcr"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi0xD6BkIWAm"
   },
   "source": [
    "# Information about dataset\n",
    "\n",
    "Let us look at datatypes and shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__fy-JxTFxc9",
    "outputId": "2be00849-30cf-470e-d3e3-6c1dabe90f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 162834 entries, 0 to 199999\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      162834 non-null  int64 \n",
      " 1   ProductId               162834 non-null  object\n",
      " 2   UserId                  162834 non-null  object\n",
      " 3   ProfileName             162834 non-null  object\n",
      " 4   HelpfulnessNumerator    162834 non-null  int64 \n",
      " 5   HelpfulnessDenominator  162834 non-null  int64 \n",
      " 6   Score                   162834 non-null  int64 \n",
      " 7   Time                    162834 non-null  int64 \n",
      " 8   Summary                 162834 non-null  object\n",
      " 9   Text                    162834 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 13.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info() #get an information about the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0xLYACiFxdJ"
   },
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.\n",
    "\n",
    "Here  we will create a dictionary that we will use for expanding the contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s6IY-x2FxdL"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JFRXFHmI7Mj"
   },
   "source": [
    "We will perform the below preprocessing tasks for our data:\n",
    "\n",
    "1.Convert everything to lowercase\n",
    "\n",
    "2.Remove HTML tags\n",
    "\n",
    "3.Contraction mapping\n",
    "\n",
    "4.Remove (‘s)\n",
    "\n",
    "5.Remove any text inside the parenthesis ( )\n",
    "\n",
    "6.Eliminate punctuations and special characters\n",
    "\n",
    "7.Remove stopwords\n",
    "\n",
    "8.Remove short words\n",
    "\n",
    "Let’s define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72oIyxJdfz2B",
    "outputId": "c8fd796a-3025-4015-f545-31266a23b603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #download Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZr-u3OEFxdT"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):  #Text cleaning function for preprocessing our raw text\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2QAeCHWFxdY"
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t,0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snRZY8wjLao2"
   },
   "source": [
    "Let us look at the first five preprocessed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCAIkhWbFxdh",
    "outputId": "f920e13c-9e05-4223-f5bf-fdb2d2cca3f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsRXocxoFxd-"
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_summary = [] #intialize empty list for storing the cleaned summary\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZeD0gs6Lnb-"
   },
   "source": [
    "Let us look at the first 10 preprocessed summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQJdZcAzFxee",
    "outputId": "ebfae2f2-eee9-4bf1-9349-3e42511f1e5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not as advertised',\n",
       " 'delight says it all',\n",
       " 'cough medicine',\n",
       " 'great taffy',\n",
       " 'nice taffy',\n",
       " 'great just as good as the expensive brands',\n",
       " 'wonderful tasty taffy',\n",
       " 'yay barley',\n",
       " 'healthy dog food']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_summary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1zLpnqsFxey"
   },
   "outputs": [],
   "source": [
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT_D2cLiLy77"
   },
   "source": [
    "#Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYK390unFxfA"
   },
   "outputs": [],
   "source": [
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm8Fk2TCL7Sp"
   },
   "source": [
    "#Understanding the distribution of the sequences\n",
    "\n",
    "Here, we will analyze the length of the Text and the summary to get an idea about the length of the text. we will then use this to fix the maximum length of the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "MdF76AHHFxgw",
    "outputId": "5cfa1133-37d2-4559-fb4c-64b347180430"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show() #plotting the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwdSGIhGMEbz"
   },
   "source": [
    "Interesting. We can fix the maximum length of the summary to 8 since that seems to be the majority summary length.\n",
    "\n",
    "Let us understand the proportion of the length of summaries below 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JRjwdIOFxg3",
    "outputId": "c2c71354-0b53-4641-989e-4d0094cf3d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943955449561134\n"
     ]
    }
   ],
   "source": [
    "unt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split())<=8):\n",
    "        unt=unt+1\n",
    "print(unt/len(data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYB4Ga9KMjEu"
   },
   "source": [
    "We observe that 94% of the summaries have length below 8. So, we can fix maximum length of summary to 8.\n",
    "\n",
    "Let us fix the maximum length of text to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKD5VOWqFxhC"
   },
   "outputs": [],
   "source": [
    "max_text_len=30 #setting the length of text \n",
    "max_summary_len=8 #setting the length of summary to be genrated to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6d48E-8M4VO"
   },
   "source": [
    "Let us select the text and summaries whose length falls below or equal to **max_text_len** and **max_summary_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY0tEJP0FxhI"
   },
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary']). #converting our Lists to numpy arrays\n",
    "\n",
    "short_text=[]  #initializing empty lists to store the summaries\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "  '''  code for selecting the text and summaries whose length falls below or equal to max_text_len and max_summary_len'''\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary}) #storing the values in a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR1uh8xSNUma"
   },
   "source": [
    "Here we are adding the **START** and **END** special tokens at the beginning and end of the summary. \n",
    "\n",
    "This is important because we are using encoder-decoder structure,this is the way the encoder knows that it has recieved an input as we are building a Seq2seq model and the encoder recieves text in a sequence.\n",
    "\n",
    "Here, for simplicity have chosen **sostok**(representing Start Of Sequence Token) and **eostok**(Representing End of Sequence Token) as START and END tokens\n",
    "\n",
    "**Note:**  these chosen special tokens never appear in the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwLUH78CFxhg"
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GlcX4RFOh13"
   },
   "source": [
    "Before building our model, we need to split our dataset into a training and validation set. We’ll use 90% of the dataset as the training data and evaluate the performance on the remaining 10% (holdout set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RakakKHcFxhl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq1mqyOHOtIl"
   },
   "source": [
    "#Preparing the Tokenizer\n",
    "\n",
    "A tokenizer builds the vocabulary and converts a word sequence to an integer sequence.\n",
    "\n",
    "#Text Tokenizer\n",
    "\n",
    "So we will build a tokenizer for our text and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRHTgX6hFxhq"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzvLwYL_PDcx"
   },
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the entire text\n",
    "\n",
    "Rare words are the words that appear less frequently than other common words\n",
    "\n",
    "Here, we will define the threshold to be 4 which means  any word whose count is below 4(i,e which appears less than 4 times) is considered as a rare word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8KronV2Fxhx",
    "outputId": "f294677e-1cd0-4985-bf57-11e5bb78d8ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 66.66472828773297\n",
      "Total Coverage of rare words: 2.1762127218939047\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "count=0\n",
    "tot_count=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_count=tot_count+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        count=count+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(count/tot_count)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So-J-5kzQIeO"
   },
   "source": [
    "\n",
    "\n",
    "* **tot_count** gives the size of vocabulary (which means every unique words in the text)\n",
    " \n",
    "*   **count** gives the no. of rare words whose count falls below threshold\n",
    "\n",
    "*  **tot_count - count** gives me the top most common words \n",
    "\n",
    "Let us define the tokenizer with top most common words for reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2giEsF3Fxh3"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCbGMsm4FxiA",
    "outputId": "ea31998b-d787-4f9d-8e86-a95c50e1edab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11466"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQfKP3sqRxi9"
   },
   "source": [
    "#Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRHqyBkBFxiJ"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KInA6O6ZSkJz"
   },
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the summary\n",
    "\n",
    "Rare words are the words that appear less frequently than other common words\n",
    "\n",
    "Here, we will define the threshold to be 4 which means  any word whose count is below 6(i,e which appears less than 4 times) is considered as a rare word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yzE5OiRLFxiM",
    "outputId": "87fdef69-8e8d-4723-b471-c648ad834acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 76.86002029822781\n",
      "Total Coverage of rare words: 3.941994057125159\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PBhzKuRSw_9"
   },
   "source": [
    "Let us define the tokenizer with top most common words for summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fswLvIgFxiR"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqwDUT5oTFmn"
   },
   "source": [
    "Let us check whether word count of start token is equal to length of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR8IX9FRFxiY",
    "outputId": "3f150f60-f20e-4186-c9e6-1d9c4e901497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78845, 78845)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVFhFVguTTtw"
   },
   "source": [
    "Here, Let's delete the rows that contain only **START** and **END** tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZ-vW82sFxih"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx5NISuMFxik"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOtlDcthFxip"
   },
   "source": [
    "# Model building\n",
    "\n",
    "Now We should build the model. But before we do that, we need to know a few terms which are required prior to building the model.\n",
    "\n",
    "**Return Sequences = True**: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\n",
    "\n",
    "**Return State = True**: When return state = True, LSTM produces the hidden state and cell state of the last timestep only\n",
    "\n",
    "**Initial State**: This is used to initialize the internal states of the LSTM for the first timestep\n",
    "\n",
    "**Stacked LSTM**: Stacked LSTM has multiple layers of LSTM stacked on top of each other. \n",
    "This leads to a better representation of the sequence.\n",
    "\n",
    "Here, we will be building a 3 stacked LSTM for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxvJKjhSnKn_",
    "outputId": "311b3139-3213-41e7-a172-d63e4823d257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keras-self-attention\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n",
      "Building wheels for collected packages: keras-self-attention\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=837ad54ac11bfb91d42ef229522a7f1b66fa0fae14c743a998022c507de9cdf1\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
      "Successfully built keras-self-attention\n",
      "Installing collected packages: keras-self-attention\n",
      "Successfully installed keras-self-attention-0.51.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXef38nBFxir",
    "outputId": "e946d33e-5db3-4655-e6fb-ae9313a17d46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 30, 100)      1146600     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 30, 300),    481200      ['embedding[0][0]']              \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 30, 300),    721200      ['lstm[0][0]']                   \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    296500      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 30, 300),    721200      ['lstm_1[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 300),  481200      ['embedding_1[0][0]',            \n",
      "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  ((None, None, 300),  180300     ['lstm_2[0][0]',                 \n",
      " r)                              (None, None, 30))                'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 600)    0           ['lstm_3[0][0]',                 \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 2965)  1781965     ['concat_layer[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,810,165\n",
      "Trainable params: 5,810,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZVlfRuMUcoP"
   },
   "source": [
    "we are using sparse categorical cross-entropy as the loss function since it converts the integer sequence to a one-hot vector on the fly. This overcomes any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwfi1Fm8Fxiz"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0ykDbxfUhyw"
   },
   "source": [
    "Remember the concept of early stopping? It is used to stop training the neural network at the right time by monitoring a user-specified metric. \n",
    "\n",
    "Here, we'll be monitoring the validation loss (val_loss). Our model will stop training once the validation loss increases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-A3J92MUljB"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw6CVECaUq5b"
   },
   "source": [
    "We’ll train the model on a batch size of 128 and validate it on the holdout set (which is 10% of our dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEFfFqAV__Tw"
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "watt81qmJ6cc"
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True) #We will use this to  make all invocations run eagerly rather than as a graph function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETnPzA4OFxi3",
    "outputId": "29a66520-4031-4835-b965-bbfd9ea73bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "604/604 [==============================] - 657s 1s/step - loss: 2.8004 - val_loss: 2.5370\n",
      "Epoch 2/10\n",
      "604/604 [==============================] - 655s 1s/step - loss: 2.4766 - val_loss: 2.3749\n",
      "Epoch 3/10\n",
      "604/604 [==============================] - 636s 1s/step - loss: 2.3480 - val_loss: 2.2856\n",
      "Epoch 4/10\n",
      "604/604 [==============================] - 636s 1s/step - loss: 2.2699 - val_loss: 2.2358\n",
      "Epoch 5/10\n",
      "604/604 [==============================] - 636s 1s/step - loss: 2.2142 - val_loss: 2.1954\n",
      "Epoch 6/10\n",
      "604/604 [==============================] - 644s 1s/step - loss: 2.1674 - val_loss: 2.1604\n",
      "Epoch 7/10\n",
      "604/604 [==============================] - 648s 1s/step - loss: 2.1296 - val_loss: 2.1364\n",
      "Epoch 8/10\n",
      "604/604 [==============================] - 656s 1s/step - loss: 2.0967 - val_loss: 2.1183\n",
      "Epoch 9/10\n",
      "604/604 [==============================] - 660s 1s/step - loss: 2.0679 - val_loss: 2.0968\n",
      "Epoch 10/10\n",
      "604/604 [==============================] - 642s 1s/step - loss: 2.0423 - val_loss: 2.0852\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=10,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0Ovi38Qvwx9"
   },
   "source": [
    "# **LOADING AND SAVING THE MODEL**\n",
    "\n",
    "As we saw Training our Model only took 2 hours of time to run on myPC since we canot always run the model whenever we want as its time consuming we can load and save the model with the weights so that it is portable and easy to us the next time we need it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "rIGbj0dzwAEl",
    "outputId": "82971cb0-0927-49aa-eadf-091f366ef7fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nLOADING THE WEIGHTS OF THE DEEP LEARNING NETWORK\\n# load json and create model\\njson_file = open(\\'model_cat.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\nloaded_model = model_from_json(loaded_model_json)\\n# load weights into new model\\nloaded_model.load_weights(\"model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "#Saving THE WEIGHTS OF THE DEEP LEARNING NETWORK\n",
    "'''\n",
    "model_save_json = model.to_json()\n",
    "with open(\"/content/drive/MyDrive/NLP/Project/model_save.json\", \"w\") as json_file:\n",
    "    json_file.write(model_save_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/content/drive/MyDrive/NLP/Project/model_save.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "'''\n",
    "#LOADING THE WEIGHTS OF THE DEEP LEARNING NETWORK\n",
    "# load json and create model\n",
    "json_file = open('model_save.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_save.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzuu8NCiwInQ"
   },
   "source": [
    "# **LOADING AND SAVING THE HISTORY OF THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvVH59mLwQIP"
   },
   "outputs": [],
   "source": [
    "#SAVING THE HISTORY\n",
    "#/content/drive/MyDrive/my_history_cat.npy -> /content/drive/MyDrive/NLP/Project/my_history_save.npy\n",
    "#np.save('/content/drive/MyDrive/NLP/Project/my_history_save.npy',history.history)\n",
    "\n",
    "#Loading the history\n",
    "\n",
    "history=np.load('/content/drive/MyDrive/NLP/Project/my_history_save.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezKYOp2UxG5"
   },
   "source": [
    "#Understanding the Diagnostic plot\n",
    "\n",
    "Now, we will plot a few diagnostic plots to understand the behavior of the model over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "tDTNLAURFxjE",
    "outputId": "142bed47-2720-4532-e994-1fd08209dbd1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSyx-HvpUz2o"
   },
   "source": [
    "From the plot, we can infer that validation loss has increased after epoch 8.\n",
    "\n",
    "Next, let’s build the dictionary to convert the index to word for target and source vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBX0zZnOFxjW"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM_nU_VvFxjq"
   },
   "source": [
    "# Inference\n",
    "\n",
    "we will Set up the inference for the encoder and decoder:\n",
    "\n",
    "The stage of growth known as deep learning inference is where the skills acquired during training are put to use. When presented with new data that the model has never seen before, the trained deep neural networks (DNN) draws conclusions or make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QkrNV-4Fxjt"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOiyk4ToWe74"
   },
   "source": [
    "We are defining a function below which is the implementation of the inference process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f6TTFnBFxj6"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GuDf4TPWt6_"
   },
   "source": [
    "NOw we'll define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAUntznIFxj9"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gM4ALyfWwA9"
   },
   "source": [
    "Here are a few summaries generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUtQmQTmFxkI",
    "outputId": "2700b63d-4a1f-4f4d-9814-d84189c94ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: delightful product buying pretty easy absolutely addicting everyone wants carmel corn product made packages last weeks yummy \n",
      "Original summary: scrumptious \n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: loves tastes good meaty cousin eat meat miss cousin put chopped cilantro onions sour cream top good time \n",
      "Original summary: love this stuff \n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  great taste\n",
      "\n",
      "\n",
      "Review: found store carry thankful amazon com son says favorite meal \n",
      "Original summary: taste great \n",
      "1/1 [==============================] - 0s 358ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: previously using recommended pharmacist worked expensive taste good decided try tea yogi since reasonably priced works well tastes much better recommend trick \n",
      "Original summary: works and tastes good \n",
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  good stuff\n",
      "\n",
      "\n",
      "Review: daughter celiac really missed kraft macaroni cheese luckily found annie loves lives ireland ship case \n",
      "Original summary: delicious mac cheese meal \n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great gluten free snack\n",
      "\n",
      "\n",
      "Review: love hot diced tomatoes hard find locally glad get thru amazon gives extra kick soups stews spaghetti sauce \n",
      "Original summary: new staple for my pantry \n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great flavor\n",
      "\n",
      "\n",
      "Review: tiny bit grainy bother bit kids really like use chocolate syrup substitute kids milk make box go really love great drinking full strength sweet love \n",
      "Original summary: this milk is really good \n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  great taste\n",
      "\n",
      "\n",
      "Review: delivered us every month love three selections personal favorite highly recommend little pricey cannot find smoked salmon locally \n",
      "Original summary: smoked salmon \n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great gift\n",
      "\n",
      "\n",
      "Review: delicious hazelnut almond filling two layers milk chocolate like gourmet peanut butter cup reasonably priced enjoyed family \n",
      "Original summary: chocolates \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: hot cocoa delicious strong enough make larger size find three flavors delicious favorite dark cocoa best bang buck far found hot cocoa tend use coffee maker everything coffee \n",
      "Original summary: well worth the money \n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  hot cocoa\n",
      "\n",
      "\n",
      "Review: check nutritional info sodium grams daily requirement used casserole ruined salty price box cost dollar unfortunately bought two boxes throwing box away bargain wildly un delicious un healthy \n",
      "Original summary: nasty even be sold no stars \n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  not the best\n",
      "\n",
      "\n",
      "Review: special treat babies pricey would think dogs would like feel look like vegetable shaped crayons full sorts goodies although feeding grain free diet first ingredient hydrolyzed wheat protein \n",
      "Original summary: my dogs love love love \n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  my dogs love these\n",
      "\n",
      "\n",
      "Review: great way start day gives lots energy feel hungry also good way keep weight control like standard special nice twist \n",
      "Original summary: great way to start the day \n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: sure really organic know amazon description say mention anywhere product organic giving review star anyways amazingly good taste probably one best tasting instant coffees \n",
      "Original summary: organic hmm \n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great for the price\n",
      "\n",
      "\n",
      "Review: received order within days heat summer packed insulated lining ice pack pretzels delicious melted seller pretzels \n",
      "Original summary: mm mm good \n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: great agave nectar complaint wish came bigger bottles use time \n",
      "Original summary: love it \n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: bully sticks smaller thought long thickness sharpie pen thought would thicker perhaps mistake reading details rate dog loves course thin last long \n",
      "Original summary: smaller than thought \n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: trying reduce caffeine intake wife likes lavazza best high quality fresh makes terrific home latte \n",
      "Original summary: best decaf pods \n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Review: winter coming drink hot sick started make hot tea lemon realized ice teas already \n",
      "Original summary: hot tea \n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great tea\n",
      "\n",
      "\n",
      "Review: flavor terrible love rest vita coco flavors one plain bad stay away \n",
      "Original summary: worst flavor \n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  awful\n",
      "\n",
      "\n",
      "Review: great find perfect christmas gifts past year flavor great stored well thanks amazon another great find \n",
      "Original summary: excellent pasta \n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great gift\n",
      "\n",
      "\n",
      "Review: cookies simply delicious fruit element overpowering way found traditional fig newtons highly recomend light snack \n",
      "Original summary: delicious \n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: best tea ever tasted blend cinnamon spices make sweet spicy usually need sugar tea tea never need sugar naturally sweet taste absolutely delightful \n",
      "Original summary: this is the best tea ever \n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great tea\n",
      "\n",
      "\n",
      "Review: love product much taste great great quality soak overnight cook order highly recommend \n",
      "Original summary: great taste \n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: love way packaged coffee several others try impressed either liked packages like coffee \n",
      "Original summary: great packaging fair coffee \n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Review: king beer prefer canned bud bottles ok drink cans without say competitor labeling beautiful near perfection \n",
      "Original summary: just love the king of \n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: diabetic looking something contains given ingredient list natural extracts way know product contains \n",
      "Original summary: ingredients \n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  not what expected\n",
      "\n",
      "\n",
      "Review: rice nice plenty spice could use cashews complaining \n",
      "Original summary: mm \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  great rice\n",
      "\n",
      "\n",
      "Review: candy best remember going store kid buying communion wafer friends glad share children edible wafer little inside sweet everyone tries seems love cannot eat one \n",
      "Original summary: just like when was kid \n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: college student needs extra energy get hour energy really gives boost need crash berry orange flavors taste great highly recommend everyone \n",
      "Original summary: great for college \n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: start amazing taste nice smell natural ingredients know try tell opinion \n",
      "Original summary: wonderful taste \n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  great taste\n",
      "\n",
      "\n",
      "Review: die best sardines ever packed olive oil green olive slices sardines huge \n",
      "Original summary: so good \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: policy use non gmo products possible cannot guarantee product produced non gmo corn low prices majority corn produced us gmo assume gmo product great amazon return policy food items stuck \n",
      "Original summary: warning they use corn for this product \n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  not worth the money\n",
      "\n",
      "\n",
      "Review: hard believe much vegetable juice fruit juice rough think could make cheaper bought however find store brand stuff walmart little cheaper good \n",
      "Original summary: tastes more like fruit than veggies \n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: daughter still loves takes snack preschool teacher tops love blend fruit vegtable super grain thrown added nutrition also easy pack playground pool \n",
      "Original summary: nutritious and delicious \n",
      "1/1 [==============================] - 0s 358ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Predicted summary:  great snack\n",
      "\n",
      "\n",
      "Review: great coffee great price added bonus feel good buying \n",
      "Original summary: great coffee \n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Review: syrup absolutely wonderful amazing authentic blackberry flavor husband uses everyday iced tea packaged well shipped promptly highly recommend product \n",
      "Original summary: monin blackberry syrup \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great syrup\n",
      "\n",
      "\n",
      "Review: husband loves pretty good moist like real jerky sense otherwise texture similar wish variety sweet definitely recommend trying getting entire package luckily got try good \n",
      "Original summary: pretty good \n",
      "1/1 [==============================] - 0s 358ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great taste\n",
      "\n",
      "\n",
      "Review: flavor okay hoping spicy cardamon clove flavor \n",
      "Original summary: mostly pepper flavor \n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: pods fit senseo machines unless use double holder even coffee like water flavor would fine stronger also several pods broken opened individual bags \n",
      "Original summary: weak \n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: order box week keep demand around slim jims far best snack food today \n",
      "Original summary: gets even better with each order \n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great snack\n",
      "\n",
      "\n",
      "Review: use ground chicken love right amount chipotle smokiness without top heat little sweetness make taco salads enchiladas burritos plain good stuff try regret \n",
      "Original summary: truly great flavor \n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: sorry person reviewed received crumbles tried every crispbread market nutritious wonderfully flavorful delicious nd place would put sunflower seed ones \n",
      "Original summary: best crispbread on the market \n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: tastes like much milder version honey nut cheerios bigger crunch say eating little hearts morning definitely staple cabinet good dry milk kids like \n",
      "Original summary: better than cheerios \n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted summary:  great snack\n",
      "\n",
      "\n",
      "Review: love product ended going recommended friends family read reviews online went ahead purchase product great litter box simple clean smell smell feeding cat wrong food try something else shocked \n",
      "Original summary: does not smell \n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: like good red beer cup zing zang oz favorite beer little dash lime salt top best red beer ever zing zang pricey bought locally totally worth \n",
      "Original summary: makes the best red beer ever \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great flavor\n",
      "\n",
      "\n",
      "Review: girlfriend found little gem tea years ago always small box enjoyed saw ginormous pack amazon figured way would go teabags great flavor nice every day tea get \n",
      "Original summary: good tea for cheap \n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great tea\n",
      "\n",
      "\n",
      "Review: sure fuss thought good coffee better store brand usually buy store brand much cheaper stick bad coffee seeing better \n",
      "Original summary: so so \n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Review: found better place buy iams lb lb amazon cheapest place buy \n",
      "Original summary: cheaper than mart \n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: used buy inspired cuisine mousse mix milk chocolate ounce boxes grocery store expensive decided give brand try since cheaper tastes love mousse try dark chocolate next \n",
      "Original summary: yum \n",
      "1/1 [==============================] - 0s 358ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "Predicted summary:  great chocolate\n",
      "\n",
      "\n",
      "Review: zevia arrived promptly perfect condition satisfied purchase would purchase good opportunity sample flavors many available retail grocery stores area \n",
      "Original summary: zevia review \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great gift\n",
      "\n",
      "\n",
      "Review: omg definitly best cereal ever breakfast snack gooood coconut whole grain good time must buy \n",
      "Original summary: best cereal ever \n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted summary:  great cereal\n",
      "\n",
      "\n",
      "Review: loved filling frosting way much made whole thing sweet wound scraping frosting layer eating sadly wasted part crust frosted would perfect buy \n",
      "Original summary: way too sweet \n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great taste\n",
      "\n",
      "\n",
      "Review: mind fine caramel creamy melting grainy flavor fine although little sweet taste \n",
      "Original summary: not creamy \n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Predicted summary:  not so good\n",
      "\n",
      "\n",
      "Review: product good great hand add hot cocoa good experience seller \n",
      "Original summary: product good seller bad \n",
      "1/1 [==============================] - 0s 358ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  hot cocoa\n",
      "\n",
      "\n",
      "Review: cannot find product store pleased able get amazon \n",
      "Original summary: pleased \n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: angel food cake mix americanized extreme full chemicals way much sugar use duncan mix find half sugar \n",
      "Original summary: sweet \n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: jersey boys knew came delight growing time child found say thank \n",
      "Original summary: best sauce ever \n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: bought first jar estate sale gone went looking could find anywhere except site good especially chicken used pieces oven highly recommend \n",
      "Original summary: chicken rub \n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: totally delicious really taste like light chocolate mousse wish stores \n",
      "Original summary: delicious \n",
      "1/1 [==============================] - 0s 354ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: know cans air buy clean things try turning one cans upside spraying poop couple seconds works great knife lifts right carpet \n",
      "Original summary: down canned air \n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: love sooo kids great texture taste kids love eat plain sauce fall apart easy cook \n",
      "Original summary: yummy \n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted summary:  great snack\n",
      "\n",
      "\n",
      "Review: kid favorite cereal life hardly keep pantry though four pack auto order perfect crushed ended mostly crumbs went back buying supermarket total bummer \n",
      "Original summary: wish this would have worked out \n",
      "1/1 [==============================] - 0s 373ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  good cereal\n",
      "\n",
      "\n",
      "Review: tea excellent flavor straight green tea happy see msg related additives ingredients listed nothing extra \n",
      "Original summary: excellent flavor \n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  tea\n",
      "\n",
      "\n",
      "Review: quit suprised big tug war toy opened two pit bulls love read stated good dogs shake throw around already purchasing one dog loves try give dog crate \n",
      "Original summary: big tug toy \n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted summary:  my dog loves these\n",
      "\n",
      "\n",
      "Review: treats time favorite hounds \n",
      "Original summary: favorite treat \n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Predicted summary:  great treats\n",
      "\n",
      "\n",
      "Review: bought sale expecting good instant oatmeal usually delicious pouch makes easy make \n",
      "Original summary: very creamy \n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great cereal\n",
      "\n",
      "\n",
      "Review: cookies taste great health conscious cookies believe two cookies added calories trans fat great taste unhealthy choice \n",
      "Original summary: great cookies with too much unhealthy stuff \n",
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great cookies\n",
      "\n",
      "\n",
      "Review: purchased item birthday party kids loved sour leave hands sticky flavor good \n",
      "Original summary: haribo sour \n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  great gift\n",
      "\n",
      "\n",
      "Review: purchased product friend loved buying may try one days \n",
      "Original summary: good stuff \n",
      "1/1 [==============================] - 0s 355ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  great\n",
      "\n",
      "\n",
      "Review: fun kitchen experience adult child children metal box good treasure box advertised baked four year old although certainly needed adult supervision experience fun us mention cupcakes delicious \n",
      "Original summary: galore \n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Predicted summary:  great gift\n",
      "\n",
      "\n",
      "Review: gave boyfriend boss christmas assumed liked wooden crate soup come attractive \n",
      "Original summary: soups on basket \n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  great gift\n",
      "\n",
      "\n",
      "Review: good shake turns sliced tomatoes salad like plain salt \n",
      "Original summary: basic for salads \n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  good but not great\n",
      "\n",
      "\n",
      "Review: month old puppy loves great training since calorie per treat according fromm website since dog small breed break treat pieces training easier chew quickly \n",
      "Original summary: puppy loves them \n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  my puppy loves these\n",
      "\n",
      "\n",
      "Review: tried one brownies one ate definitely raspberry flavor brownie absolutely delicious rich taste raspberry maybe taste ones \n",
      "Original summary: raspberry \n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted summary:  best chocolate ever\n",
      "\n",
      "\n",
      "Review: always look forward home treats could picked elsewhere locations told cheaper time arrived time scheduled enjoyed family kids looking forward making coworkers soon good stuff wait get hands road \n",
      "Original summary: mix \n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great for training\n",
      "\n",
      "\n",
      "Review: love shampoo hair silky smells wonderful gone one washing color hair color brighter dull color rinsed washing definate thumbs \n",
      "Original summary: shampoo \n",
      "1/1 [==============================] - 0s 371ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: say mary candy time favorite wish stores carried shipping product fast \n",
      "Original summary: the best old fashion candy \n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great candy\n",
      "\n",
      "\n",
      "Review: great dog favorite seem healthy snack easy cut half want give smaller portion \n",
      "Original summary: dog is nuts about \n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  great treat\n",
      "\n",
      "\n",
      "Review: bought cheese mom loved would buy \n",
      "Original summary: awesome cheese \n",
      "1/1 [==============================] - 0s 363ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  yummy\n",
      "\n",
      "\n",
      "Review: like every dr product tried great bought wholefoods sale ended per box oz cheaper get terms product quality great price may want consider buying store \n",
      "Original summary: great taste and texture \n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: really enjoy syrup wonderful flavor purchase \n",
      "Original summary: satisfied customer \n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great syrup\n",
      "\n",
      "\n",
      "Review: using similac advanced formula thought happy thought switching organic knowing baby getting little less exposure hormones chemicals regular formula worth \n",
      "Original summary: only the best \n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: read several reviews product gave tried tried like work several bites even though know packed great proteins low calories going save emergency purposes daily workout \n",
      "Original summary: still had that smell \n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: remember trying one child spicy spat instantly ginger flavor overwhelmingly strong people seem love though suit palate \n",
      "Original summary: spicy \n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  not good\n",
      "\n",
      "\n",
      "Review: kids aged love cereal love sugary cinnamon toast cereals husband occasional bowl well loves love cereal \n",
      "Original summary: love this cereal \n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great cereal\n",
      "\n",
      "\n",
      "Review: love kcups especially delicious green mountain nantucket blends however needs recycling plan put place quick every time use one feel bad \n",
      "Original summary: delicious coffee wish they were \n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great coffee\n",
      "\n",
      "\n",
      "Review: hard find aluminum free baking powder local store small packages shipping makes price bit high convienient since buy bulk \n",
      "Original summary: baking powder \n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: first time purchased particular brand vanilla normally would purchase large bottle found flavor wonderful glad bought like bake size bottle keep busy \n",
      "Original summary: vanilla flavor not weak as some are \n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: plus grams protein bars smaller bars bland taste even different flavors taste much better choice clif protein bars also gms protein larger taste great \n",
      "Original summary: do not waste your money \n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  great snack\n",
      "\n",
      "\n",
      "Review: product arrived much sooner expected taste exactly like simple syrup could tell sugar free perfect diabetic simply counting calories wish ordered one bottle \n",
      "Original summary: sugar free simple syrup \n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great syrup\n",
      "\n",
      "\n",
      "Review: cups brew excellent cup decaf tea wonderful fresh brewed taste seconds keurig brewer disappointed \n",
      "Original summary: twinings english breakfast tea \n",
      "1/1 [==============================] - 0s 365ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great decaf\n",
      "\n",
      "\n",
      "Review: gluten free oatmeal delicious always think like apple cinnamon flavor better fast shipping exceptional pricing \n",
      "Original summary: delicious \n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great gluten free snack\n",
      "\n",
      "\n",
      "Review: grandma cookies tasty enough eating healthy try \n",
      "Original summary: good quick snack or breakfast \n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  great cookies\n",
      "\n",
      "\n",
      "Review: crunch granola bar par every product clif name would imply crunch extremely delicious favorite granola bar nature valley definitely took cake simply put delicious \n",
      "Original summary: clif never me down \n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted summary:  great snack\n",
      "\n",
      "\n",
      "Review: horlicks used used making malts old days however sold company place england changed taste taste good making old fashioned malt hard mix hand however make good drink milk hot cold \n",
      "Original summary: different taste but good \n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted summary:  great product\n",
      "\n",
      "\n",
      "Review: ordered lavender use ice cream stems leaves husks strain ice cream lavender flavor cooking spreading two tablespoons trying pick dried flowers gave realizing non flower debris total \n",
      "Original summary: poor quality full of leaves and \n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted summary:  not worth the money\n",
      "\n",
      "\n",
      "Review: maybe dont like couscous stuff taste good rather hardy made hard get taste \n",
      "Original summary: yuk \n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted summary:  not bad\n",
      "\n",
      "\n",
      "Review: stuff delicious love already ordered second round right amount spice \n",
      "Original summary: love this stuff \n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Predicted summary:  delicious\n",
      "\n",
      "\n",
      "Review: need biscuit fix good costs much better way get book carb sugar new fat use cooked gluten flour secret make much less money \n",
      "Original summary: nice but too pricey \n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted summary:  good but not great\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTkaYNjHW4lC"
   },
   "source": [
    " Even though the actual summary and the summary generated by our model do not match in terms of words, both of them are conveying the same meaning. Our model is able to generate a clear summary based on the context present in the text.\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkqtbEHYTKUC"
   },
   "source": [
    "#Conclusion\n",
    "\n",
    "Here we have built our own Deep Learning Model from Scratch, we can still do a lot of things to improve the performance of the model such as \n",
    "\n",
    "1) Training with more dataset\n",
    "\n",
    "2) implementing a Bi-Directional LSTM model\n",
    "\n",
    "and many more\n",
    "\n",
    "But that's all for another day."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
